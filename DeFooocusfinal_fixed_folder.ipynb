{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Filarh/Hello-World/blob/master/DeFooocusfinal_fixed_folder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWA1uOTLmDUf",
        "outputId": "bca68def-e384-4173-b6ee-8f5b8a3a9b2f",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Fetched 1,513 kB in 0s (7,132 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 124788 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Collecting pygit2==1.12.2\n",
            "  Downloading pygit2-1.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: cffi>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from pygit2==1.12.2) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.9.1->pygit2==1.12.2) (2.22)\n",
            "Downloading pygit2-1.12.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygit2\n",
            "  Attempting uninstall: pygit2\n",
            "    Found existing installation: pygit2 1.16.0\n",
            "    Uninstalling pygit2-1.16.0:\n",
            "      Successfully uninstalled pygit2-1.16.0\n",
            "Successfully installed pygit2-1.12.2\n",
            "/content\n",
            "Cloning into 'Fooocus'...\n",
            "remote: Enumerating objects: 6725, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6725 (delta 2), reused 0 (delta 0), pack-reused 6721 (from 3)\u001b[K\n",
            "Receiving objects: 100% (6725/6725), 33.27 MiB | 24.26 MiB/s, done.\n",
            "Resolving deltas: 100% (3875/3875), done.\n",
            "/content/Fooocus\n",
            "Mounted at /content/drive\n",
            "\u001b[92mUsing default.json as a reference\u001b[0m\n",
            "Created ZavyChromaXL.json with the provided parameters and structure!\n",
            "\u001b[94mDownloading from URL: aria2c -x 16 -s 16 -k 1M \"https://civitai.com/api/download/models/916744?type=Model&format=SafeTensor&size=full&fp=fp16\" -d \"/content/Fooocus/models/checkpoints\" -o \"ZavyChromaXL.safetensors\"\u001b[0m\n",
            "\u001b[92mDownloaded file at: /content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors\u001b[0m\n",
            "\u001b[92mModel downloaded: /content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors\u001b[0m\n",
            "\u001b[95m[DeFooocus] Preparing...\u001b[0m\n",
            "\u001b[95m[DeFooocus] Starting...\u001b[0m\n",
            "Already up-to-date\n",
            "Update succeeded.\n",
            "[System ARGV] ['entry_with_update.py', '--share', '--attention-split', '--always-high-vram', '--disable-offload-from-vram', '--all-in-fp16', '--theme', 'dark', '--preset', 'ZavyChromaXL']\n",
            "Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "Fooocus version: 2.5.5\n",
            "Error checking version for torchsde: No package metadata was found for torchsde\n",
            "Installing requirements\n",
            "Loaded preset: /content/Fooocus/presets/ZavyChromaXL.json\n",
            "[Cleanup] Attempting to delete content of temp dir /tmp/fooocus\n",
            "[Cleanup] Cleanup successful\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/xlvaeapp.pth\" to /content/Fooocus/models/vae_approx/xlvaeapp.pth\n",
            "\n",
            "100% 209k/209k [00:00<00:00, 22.1MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/vaeapp_sd15.pt\" to /content/Fooocus/models/vae_approx/vaeapp_sd15.pth\n",
            "\n",
            "100% 209k/209k [00:00<00:00, 16.6MB/s]\n",
            "Downloading: \"https://huggingface.co/mashb1t/misc/resolve/main/xl-to-v1_interposer-v4.0.safetensors\" to /content/Fooocus/models/vae_approx/xl-to-v1_interposer-v4.0.safetensors\n",
            "\n",
            "100% 5.40M/5.40M [00:00<00:00, 50.9MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_expansion.bin\" to /content/Fooocus/models/prompt_expansion/fooocus_expansion/pytorch_model.bin\n",
            "\n",
            "100% 335M/335M [00:02<00:00, 130MB/s] \n",
            "Total VRAM 15102 MB, total RAM 12979 MB\n",
            "Forcing FP16.\n",
            "Set vram state to: HIGH_VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype: torch.float32\n",
            "Using split optimization for cross attention\n",
            "Refiner unloaded.\n",
            "Running on local URL:  http://127.0.0.1:7865\n",
            "IMPORTANT: You are using gradio version 3.41.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://c35d78a0448b147ea5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using split attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using split attention in VAE\n",
            "extra {'cond_stage_model.clip_l.text_projection', 'cond_stage_model.clip_l.logit_scale'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [('sd_xl_offset_example-lora_1.0.safetensors', 0.1)] for model [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors].\n",
            "Loaded LoRA [/content/drive/.shortcut-targets-by-id/19w0mqvfYN62Fnn4zuWImdyxaPu7rnWGN/MyLoras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors] with 788 keys at weight 0.1.\n",
            "Fooocus V2 Expansion: Vocab with 642 words.\n",
            "Fooocus Expansion engine loaded for cuda:0, use_fp16 = True.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.75 seconds\n",
            "2025-01-28 15:47:17.157096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-28 15:47:17.420481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-28 15:47:17.487776: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-28 15:47:17.897579: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-28 15:47:20.127156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Started worker with PID 958\n",
            "App started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865 or https://c35d78a0448b147ea5.gradio.live\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 6003166055716394630\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Request to load LoRAs [('mrblnrvdnr.safetensors', 1.0)] for model [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors].\n",
            "Loaded LoRA [/content/drive/.shortcut-targets-by-id/19w0mqvfYN62Fnn4zuWImdyxaPu7rnWGN/MyLoras/mrblnrvdnr.safetensors] for UNet [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors] with 722 keys at weight 1.0.\n",
            "Loaded LoRA [/content/drive/.shortcut-targets-by-id/19w0mqvfYN62Fnn4zuWImdyxaPu7rnWGN/MyLoras/mrblnrvdnr.safetensors] for CLIP [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors] with 264 keys at weight 1.0.\n",
            "Requested to load SDXLClipModel\n",
            "Loading 1 new model\n",
            "unload clone 1\n",
            "[Fooocus Model Management] Moving model(s) has taken 2.03 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1216, 832)\n",
            "Preparation time: 12.28 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "unload clone 2\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.41 seconds\n",
            "100% 30/30 [00:47<00:00,  1.57s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 51.67 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.78 seconds\n",
            "100% 30/30 [00:46<00:00,  1.56s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.32 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 50.55 seconds\n",
            "[Fooocus] Processing enhance ...\n",
            "[Fooocus] Downloading upscale models ...\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_upscaler_s409985e5.bin\" to /content/Fooocus/models/upscale_models/fooocus_upscaler_s409985e5.bin\n",
            "\n",
            "100% 32.1M/32.1M [00:00<00:00, 218MB/s]\n",
            "[Fooocus] Upscaling image from (832, 1216) ...\n",
            "Upscaling image with shape (1216, 832, 3) ...\n",
            "Image upscaled.\n",
            "[Fooocus] VAE encoding ...\n",
            "Final resolution is (1248, 1824).\n",
            "[Fooocus] Preparing enhance prompts ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.86 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 0.5689725279808044\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.41 seconds\n",
            "100% 18/18 [01:56<00:00,  6.45s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.21 seconds\n",
            "[Fooocus] Saving image 1/4 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "[Fooocus] Preparing enhancement 2/4 ...\n",
            "[Enhance] Searching for \"face\"\n",
            "Downloading: \"https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\" to /content/Fooocus/models/inpaint/groundingdino_swint_ogc.pth\n",
            "\n",
            "100% 662M/662M [00:03<00:00, 226MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "final text_encoder_type: bert-base-uncased\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 345kB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.59MB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 1.43MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.81MB/s]\n",
            "model.safetensors: 100% 440M/440M [00:04<00:00, 92.2MB/s]\n",
            "/content/Fooocus/modules/patch.py:465: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  result = original_loader(*args, **kwargs)\n",
            "Requested to load GroundingDINO\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.23 seconds\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:1060: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/groundingdino/models/GroundingDINO/transformer.py:862: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n",
            "Downloading: \"https://huggingface.co/mashb1t/misc/resolve/main/sam_vit_b_01ec64.pth\" to /content/Fooocus/models/sam/sam_vit_b_01ec64.pth\n",
            "\n",
            "100% 358M/358M [00:08<00:00, 42.6MB/s]\n",
            "Requested to load Sam\n",
            "Loading 1 new model\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1372, in handler\n",
            "    mask, dino_detection_count, sam_detection_count, sam_detection_on_mask_count = generate_mask_from_image(\n",
            "                                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/extras/inpaint_mask.py\", line 91, in generate_mask_from_image\n",
            "    sam_predictor.set_image(image)\n",
            "  File \"/content/Fooocus/extras/sam/predictor.py\", line 71, in set_image\n",
            "    self.set_torch_image(input_image_torch, image.shape[:2])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/extras/sam/predictor.py\", line 101, in set_torch_image\n",
            "    self.features = self.patcher.model.image_encoder(input_image)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 112, in forward\n",
            "    x = blk(x)\n",
            "        ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 174, in forward\n",
            "    x = self.attn(x)\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 234, in forward\n",
            "    attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 358, in add_decomposed_rel_pos\n",
            "    attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 553.06 MiB is free. Process 8258 has 14.21 GiB memory in use. Of the allocated memory 11.99 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Total time: 309.97 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 2086934338843530069\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Request to load LoRAs [('mrblnrvdnr.safetensors', 1.1)] for model [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors].\n",
            "Loaded LoRA [/content/drive/.shortcut-targets-by-id/19w0mqvfYN62Fnn4zuWImdyxaPu7rnWGN/MyLoras/mrblnrvdnr.safetensors] for UNet [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors] with 722 keys at weight 1.1.\n",
            "Loaded LoRA [/content/drive/.shortcut-targets-by-id/19w0mqvfYN62Fnn4zuWImdyxaPu7rnWGN/MyLoras/mrblnrvdnr.safetensors] for CLIP [/content/Fooocus/models/checkpoints/ZavyChromaXL.safetensors] with 264 keys at weight 1.1.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.90 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding positive #3 ...\n",
            "[Fooocus] Encoding positive #4 ...\n",
            "[Fooocus] Encoding positive #5 ...\n",
            "[Fooocus] Encoding positive #6 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Encoding negative #3 ...\n",
            "[Fooocus] Encoding negative #4 ...\n",
            "[Fooocus] Encoding negative #5 ...\n",
            "[Fooocus] Encoding negative #6 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1216, 832)\n",
            "Preparation time: 4.34 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "unload clone 5\n",
            "[Fooocus Model Management] Moving model(s) has taken 2.61 seconds\n",
            "100% 30/30 [00:47<00:00,  1.57s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.28 seconds\n",
            "[Fooocus] Saving image 1/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 52.50 seconds\n",
            "[Fooocus] Preparing task 2/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.77 seconds\n",
            "100% 30/30 [00:47<00:00,  1.60s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.36 seconds\n",
            "[Fooocus] Saving image 2/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 51.67 seconds\n",
            "[Fooocus] Preparing task 3/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.83 seconds\n",
            "100% 30/30 [00:48<00:00,  1.61s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.31 seconds\n",
            "[Fooocus] Saving image 3/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 51.89 seconds\n",
            "[Fooocus] Preparing task 4/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.74 seconds\n",
            "100% 30/30 [00:48<00:00,  1.62s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.36 seconds\n",
            "[Fooocus] Saving image 4/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 52.22 seconds\n",
            "[Fooocus] Preparing task 5/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.74 seconds\n",
            "100% 30/30 [00:48<00:00,  1.62s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\n",
            "[Fooocus] Saving image 5/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 52.08 seconds\n",
            "[Fooocus] Preparing task 6/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.86 seconds\n",
            "100% 30/30 [00:48<00:00,  1.62s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\n",
            "[Fooocus] Saving image 6/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 52.12 seconds\n",
            "[Fooocus] Processing enhance ...\n",
            "[Fooocus] Downloading upscale models ...\n",
            "[Fooocus] Upscaling image from (832, 1216) ...\n",
            "Upscaling image with shape (1216, 832, 3) ...\n",
            "Image upscaled.\n",
            "[Fooocus] VAE encoding ...\n",
            "Final resolution is (1248, 1824).\n",
            "[Fooocus] Preparing enhance prompts ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.78 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 0.5689725279808044\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.25 seconds\n",
            "100% 18/18 [01:57<00:00,  6.51s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.11 seconds\n",
            "[Fooocus] Saving image 1/12 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "[Fooocus] Preparing enhancement 2/12 ...\n",
            "[Enhance] Searching for \"face\"\n",
            "Requested to load GroundingDINO\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\n",
            "Requested to load Sam\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.10 seconds\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1372, in handler\n",
            "    mask, dino_detection_count, sam_detection_count, sam_detection_on_mask_count = generate_mask_from_image(\n",
            "                                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/extras/inpaint_mask.py\", line 91, in generate_mask_from_image\n",
            "    sam_predictor.set_image(image)\n",
            "  File \"/content/Fooocus/extras/sam/predictor.py\", line 71, in set_image\n",
            "    self.set_torch_image(input_image_torch, image.shape[:2])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/extras/sam/predictor.py\", line 101, in set_torch_image\n",
            "    self.features = self.patcher.model.image_encoder(input_image)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 112, in forward\n",
            "    x = blk(x)\n",
            "        ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 174, in forward\n",
            "    x = self.attn(x)\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 234, in forward\n",
            "    attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/segment_anything/modeling/image_encoder.py\", line 358, in add_decomposed_rel_pos\n",
            "    attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n",
            "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 455.06 MiB is free. Process 8258 has 14.30 GiB memory in use. Of the allocated memory 12.09 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "Total time: 489.05 seconds\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 9196823207384134045\n",
            "[Parameters] CFG = 4\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.78 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding positive #3 ...\n",
            "[Fooocus] Encoding positive #4 ...\n",
            "[Fooocus] Encoding positive #5 ...\n",
            "[Fooocus] Encoding positive #6 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Encoding negative #3 ...\n",
            "[Fooocus] Encoding negative #4 ...\n",
            "[Fooocus] Encoding negative #5 ...\n",
            "[Fooocus] Encoding negative #6 ...\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1216, 832)\n",
            "Preparation time: 1.25 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.16 seconds\n",
            "100% 30/30 [00:48<00:00,  1.63s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.27 seconds\n",
            "[Fooocus] Saving image 1/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 52.75 seconds\n",
            "[Fooocus] Preparing task 2/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.84 seconds\n",
            "100% 30/30 [00:49<00:00,  1.66s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\n",
            "[Fooocus] Saving image 2/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 53.43 seconds\n",
            "[Fooocus] Preparing task 3/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.67 seconds\n",
            "100% 30/30 [00:50<00:00,  1.68s/it]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.28 seconds\n",
            "[Fooocus] Saving image 3/6 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Generating and saving time: 53.83 seconds\n",
            "[Fooocus] Preparing task 4/6 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.71 seconds\n",
            " 17% 5/30 [00:10<00:50,  2.02s/it]\n",
            "User stopped\n",
            "[Fooocus] Processing enhance ...\n",
            "[Fooocus] Downloading upscale models ...\n",
            "[Fooocus] Upscaling image from (832, 1216) ...\n",
            "Upscaling image with shape (1216, 832, 3) ...\n",
            "Image upscaled.\n",
            "[Fooocus] Saving image 1/3 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Enhancement image time: 17.51 seconds\n",
            "[Fooocus] Downloading upscale models ...\n",
            "[Fooocus] Upscaling image from (832, 1216) ...\n",
            "Upscaling image with shape (1216, 832, 3) ...\n",
            "Image upscaled.\n",
            "[Fooocus] Saving image 2/3 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Enhancement image time: 17.30 seconds\n",
            "[Fooocus] Downloading upscale models ...\n",
            "[Fooocus] Upscaling image from (832, 1216) ...\n",
            "Upscaling image with shape (1216, 832, 3) ...\n",
            "Image upscaled.\n",
            "[Fooocus] Saving image 3/3 to system ...\n",
            "Image generated with private log at: /content/drive/MyDrive/Fooocus_output/2025-01-28/log.html\n",
            "Enhancement image time: 17.31 seconds\n",
            "Processing time (total): 222.97 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 224.52 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.93 seconds\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2199, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/entry_with_update.py\", line 46, in <module>\n",
            "    from launch import *\n",
            "  File \"/content/Fooocus/launch.py\", line 171, in <module>\n",
            "    from webui import *\n",
            "  File \"/content/Fooocus/webui.py\", line 1120, in <module>\n",
            "    shared.gradio_root.launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2115, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2203, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/networking.py\", line 49, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7865 <> https://c35d78a0448b147ea5.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# @title 🎨 DeFooocus Setup\n",
        "# @markdown ### Configuración básica\n",
        "\n",
        "# @markdown ### Selección de versión\n",
        "version = \"Fooocus\"  # @param [\"DeFooocus\", \"Fooocus\"]\n",
        "\n",
        "activar_drive = True  # @param {type:\"boolean\"}\n",
        "modificar_modelo = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown ### Configuración del modelo\n",
        "# Se utiliza un parámetro desplegable para elegir el preset deseado.\n",
        "modelo_seleccion = \"SDVN8-ArtXL (Arte y vectores)\"  # @param [\"ZavyChromaXL_v10 (realismo, fotos)\", \"SDVN8-ArtXL (Arte y vectores)\", \"Custom\"]\n",
        "custom_link = \"\"  # @param {type:\"string\"}  # Se utiliza solo si se elige \"Custom\"\n",
        "\n",
        "aspect_ratio = \"896*1216\"  # @param {type:\"string\"}\n",
        "formato = \"safetensors\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Configuración de tokens (opcional)\n",
        "tokenCIVITAI = \"\"  # @param {type:\"string\"}\n",
        "tokenHUGGINGFACE = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Configuración de la interfaz\n",
        "theme = \"dark\" # @param [\"dark\", \"light\"]\n",
        "preset = \"default\"  # @param [\"default\", \"anime\", \"dpo\", \"hypersd\", \"lcm\", \"lightning\", \"playground_v2.5\", \"realistic\", \"sai\", \"sd1.5\", \"spo\", \"turbo\"]\n",
        "advenced_args = \"--share --attention-split --always-high-vram --disable-offload-from-vram --all-in-fp16\"  # @param {type:\"string\"}\n",
        "\n",
        "import re\n",
        "\n",
        "# Determinar el enlace y nombre del modelo en función de la opción seleccionada\n",
        "if modelo_seleccion == \"Custom\":\n",
        "    if custom_link.strip() == \"\":\n",
        "        raise ValueError(\"Para el modelo Custom, por favor proporciona un enlace en 'custom_link'.\")\n",
        "    else:\n",
        "        link_archivo = custom_link.strip()\n",
        "        is_custom = True\n",
        "        nombre_archivo = \"\"  # Se obtendrá luego mediante aria2c\n",
        "else:\n",
        "    is_custom = False\n",
        "    if modelo_seleccion == \"ZavyChromaXL_v10 (realismo, fotos)\":\n",
        "        link_archivo = \"https://civitai.com/api/download/models/916744?type=Model&format=SafeTensor&size=full&fp=fp16\"\n",
        "    elif modelo_seleccion == \"SDVN8-ArtXL (Arte y vectores)\":\n",
        "        link_archivo = \"https://civitai.com/api/download/models/212479?type=Model&format=SafeTensor&size=full&fp=fp16\"\n",
        "    # Se elimina cualquier contenido entre paréntesis y espacios para el nombre del preset (nombre interno para el archivo)\n",
        "    nombre_archivo = re.sub(r'\\s*\\(.*?\\)', '', modelo_seleccion).replace(\" \", \"\")\n",
        "\n",
        "# Check if 'Args' is not in the local scope\n",
        "if 'Args' not in locals():\n",
        "    # Install dependencies if necessary\n",
        "    !apt install -y aria2\n",
        "    !pip install pygit2==1.12.2\n",
        "    %cd /content\n",
        "\n",
        "    # Clone the selected version\n",
        "    if version == \"DeFooocus\":\n",
        "        !git clone https://github.com/ehristoforu/DeFooocus.git\n",
        "        %cd /content/DeFooocus\n",
        "    else:\n",
        "        !git clone https://github.com/lllyasviel/Fooocus.git\n",
        "        %cd /content/Fooocus\n",
        "\n",
        "    Args = \"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# Fix: Create the 'cached_download' folder in the 'huggingface_hub' path if it doesn't exist\n",
        "import sys\n",
        "huggingface_hub_path = '/usr/local/lib/python3.10/dist-packages/huggingface_hub'\n",
        "cached_download_path = os.path.join(huggingface_hub_path, \"cached_download\")\n",
        "if not os.path.exists(cached_download_path):\n",
        "    os.makedirs(cached_download_path)\n",
        "\n",
        "# Function to print in color (Colab compatible)\n",
        "def print_colored(text, color):\n",
        "    color_codes = {\n",
        "        \"red\": \"\\033[91m\",\n",
        "        \"green\": \"\\033[92m\",\n",
        "        \"yellow\": \"\\033[93m\",\n",
        "        \"blue\": \"\\033[94m\",\n",
        "        \"magenta\": \"\\033[95m\",\n",
        "        \"cyan\": \"\\033[96m\",\n",
        "        \"white\": \"\\033[97m\",\n",
        "        \"reset\": \"\\033[0m\"\n",
        "    }\n",
        "    print(f\"{color_codes.get(color, color_codes['reset'])}{text}{color_codes['reset']}\")\n",
        "\n",
        "# Function to modify JSON with provided parameters\n",
        "def modificar_json(nombre_archivo: str, link_archivo: str, aspect_ratio: str, formato: str):\n",
        "    carpeta = '/content/DeFooocus/presets' if version == \"DeFooocus\" else '/content/Fooocus/presets'\n",
        "    archivos = os.listdir(carpeta)\n",
        "\n",
        "    json_encontrado = None\n",
        "    if 'default.json' in archivos:\n",
        "        json_encontrado = 'default.json'\n",
        "        print_colored(\"Using default.json as a reference\", \"green\")\n",
        "    else:\n",
        "        json_encontrado = next((archivo for archivo in archivos if archivo.endswith('.json')), None)\n",
        "        print_colored(\"No default.json found, using a random JSON file.\", \"yellow\")\n",
        "\n",
        "    if json_encontrado:\n",
        "        with open(os.path.join(carpeta, json_encontrado)) as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        archivo_modelo = f\"{nombre_archivo}.{formato}\"\n",
        "        data['default_model'] = archivo_modelo\n",
        "        data['default_aspect_ratio'] = aspect_ratio\n",
        "        data['default_refiner'] = \"\"\n",
        "\n",
        "        data['checkpoint_downloads'] = {archivo_modelo: link_archivo}\n",
        "\n",
        "        global Args\n",
        "        Args = f\"--preset {nombre_archivo}\"\n",
        "\n",
        "        nuevo_nombre = f\"{nombre_archivo}.json\"\n",
        "        with open(os.path.join(carpeta, nuevo_nombre), 'w') as file:\n",
        "            json.dump(data, file, separators=(',', ':'))\n",
        "\n",
        "        return f\"Created {nuevo_nombre} with the provided parameters and structure!\"\n",
        "    else:\n",
        "        return \"No JSON files found in the specified folder.\"\n",
        "\n",
        "# Function to download the model using aria2 if it does not exist.\n",
        "# Se añade el parámetro is_custom para saber si se debe dejar que aria2c asigne el nombre automáticamente.\n",
        "def download_model_aria2(url, nombre_archivo, formato, tokenCIVITAI, tokenHUGGINGFACE, is_custom=False):\n",
        "    try:\n",
        "        model_dir = \"/content/DeFooocus/models/checkpoints\" if version == \"DeFooocus\" else \"/content/Fooocus/models/checkpoints\"\n",
        "        if not os.path.exists(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "\n",
        "        if not is_custom:\n",
        "            archivo_modelo = f\"{nombre_archivo}.{formato}\"\n",
        "            destino = os.path.join(model_dir, archivo_modelo)\n",
        "\n",
        "            if os.path.exists(destino):\n",
        "                print_colored(f\"File {archivo_modelo} already exists. No download required.\", \"cyan\")\n",
        "                return destino, archivo_modelo\n",
        "\n",
        "            # Append token if needed\n",
        "            if \"huggingface\" in url and tokenHUGGINGFACE:\n",
        "                url += f\"&token={tokenHUGGINGFACE}\"\n",
        "            elif \"civitai\" in url and tokenCIVITAI:\n",
        "                url += f\"&token={tokenCIVITAI}\"\n",
        "\n",
        "            download_command = f'aria2c -x 16 -s 16 -k 1M \"{url}\" -d \"{model_dir}\" -o \"{archivo_modelo}\"'\n",
        "            print_colored(f\"Downloading from URL: {download_command}\", \"blue\")\n",
        "            os.system(download_command)\n",
        "\n",
        "            if os.path.exists(destino):\n",
        "                print_colored(f\"Downloaded file at: {destino}\", \"green\")\n",
        "                return destino, archivo_modelo\n",
        "            else:\n",
        "                print_colored(\"Failed to download file. ¿si agregaste la clave API?\", \"red\")\n",
        "                return None, None\n",
        "        else:\n",
        "            # Para descarga custom: no se especifica nombre (flag -o) y se obtiene el nombre descargado\n",
        "            existing_files = set(os.listdir(model_dir))\n",
        "\n",
        "            if \"huggingface\" in url and tokenHUGGINGFACE:\n",
        "                url += f\"&token={tokenHUGGINGFACE}\"\n",
        "            elif \"civitai\" in url and tokenCIVITAI:\n",
        "                url += f\"&token={tokenCIVITAI}\"\n",
        "\n",
        "            download_command = f'aria2c -x 16 -s 16 -k 1M \"{url}\" -d \"{model_dir}\"'\n",
        "            print_colored(f\"Downloading (custom) from URL: {download_command}\", \"blue\")\n",
        "            os.system(download_command)\n",
        "\n",
        "            new_files = set(os.listdir(model_dir)) - existing_files\n",
        "            new_files = [f for f in new_files if f.endswith(f\".{formato}\")]\n",
        "            if len(new_files) == 0:\n",
        "                print_colored(\"Failed to download file. ¿si agregaste la clave API?\", \"red\")\n",
        "                return None, None\n",
        "            elif len(new_files) == 1:\n",
        "                archivo_modelo = new_files[0]\n",
        "            else:\n",
        "                # Si se descargaron varios, se elige el más reciente\n",
        "                archivo_modelo = sorted(new_files, key=lambda f: os.path.getmtime(os.path.join(model_dir, f)), reverse=True)[0]\n",
        "            destino = os.path.join(model_dir, archivo_modelo)\n",
        "            print_colored(f\"Downloaded file at: {destino}\", \"green\")\n",
        "            return destino, archivo_modelo\n",
        "\n",
        "    except Exception as e:\n",
        "        print_colored(f\"Download error: {e}\", \"red\")\n",
        "        return None, None\n",
        "\n",
        "# Activating Google Drive if necessary\n",
        "if activar_drive:\n",
        "    drive.mount('/content/drive')\n",
        "    nuevo_dir_lora = '/content/drive/MyDrive/MyLoras'\n",
        "    nuevo_dir_salida = '/content/drive/MyDrive/Fooocus_output'\n",
        "\n",
        "    launch_py_path = '/content/DeFooocus/launch.py' if version == \"DeFooocus\" else '/content/Fooocus/launch.py'\n",
        "\n",
        "    if os.path.exists(launch_py_path):\n",
        "        with open(launch_py_path, 'r') as file:\n",
        "            launch_py_content = file.read()\n",
        "\n",
        "        codigo_a_insertar = f'''\n",
        "import os\n",
        "import json\n",
        "\n",
        "nuevo_dir_lora = '{nuevo_dir_lora}'\n",
        "nuevo_dir_salida = '{nuevo_dir_salida}'\n",
        "\n",
        "config_path = os.path.abspath(\"./config.txt\")\n",
        "config_dict = {{}}\n",
        "\n",
        "if os.path.exists(config_path):\n",
        "    with open(config_path, \"r\", encoding=\"utf-8\") as json_file:\n",
        "        config_dict = json.load(json_file)\n",
        "\n",
        "config_dict['path_loras'] = os.path.abspath(nuevo_dir_lora)\n",
        "config_dict['path_outputs'] = os.path.abspath(nuevo_dir_salida)\n",
        "\n",
        "with open(config_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(config_dict, json_file, indent=4)\n",
        "'''\n",
        "\n",
        "        if codigo_a_insertar not in launch_py_content:\n",
        "            with open(launch_py_path, 'w') as file:\n",
        "                file.write(codigo_a_insertar + launch_py_content)\n",
        "        else:\n",
        "            print_colored(\"Code already inserted in launch.py\", \"cyan\")\n",
        "    else:\n",
        "        print_colored(\"launch.py file not found.\", \"red\")\n",
        "\n",
        "# Se descarga primero el modelo; en el caso custom se obtendrá el nombre descargado.\n",
        "model_url = link_archivo\n",
        "model_sd, downloaded_filename = download_model_aria2(model_url, nombre_archivo, formato, tokenCIVITAI, tokenHUGGINGFACE, is_custom=is_custom)\n",
        "if model_sd:\n",
        "    print_colored(f\"Model downloaded: {model_sd}\", \"green\")\n",
        "    if is_custom:\n",
        "        # Se utiliza el nombre obtenido del archivo, quitándole la extensión y eliminando espacios\n",
        "        nombre_archivo = os.path.splitext(downloaded_filename)[0].replace(\" \", \"\")\n",
        "else:\n",
        "    print_colored(\"Model download failed.\", \"red\")\n",
        "\n",
        "# Si se requiere modificar el modelo (JSON), se actualiza con el nombre final y el enlace original.\n",
        "if modificar_modelo:\n",
        "    resultado = modificar_json(nombre_archivo, link_archivo, aspect_ratio, formato)\n",
        "    print(resultado)\n",
        "\n",
        "print_colored(\"[DeFooocus] Preparing...\", \"magenta\")\n",
        "\n",
        "if preset != \"default\":\n",
        "    args = f\"{advenced_args} --theme {theme} --preset {preset}\"\n",
        "else:\n",
        "    args = f\"{advenced_args} --theme {theme}\"\n",
        "\n",
        "args = f\"{args} {Args}\"\n",
        "\n",
        "print_colored(\"[DeFooocus] Starting...\", \"magenta\")\n",
        "!python entry_with_update.py {args}\n"
      ]
    }
  ]
}